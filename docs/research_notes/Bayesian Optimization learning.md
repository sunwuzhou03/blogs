# Bayesian Optimization learning

# bayesian optimization的基本流程
贝叶斯优化是一种高效的全局优化策略，特别适用于那些评估成本高昂的黑箱函数。以下是贝叶斯优化的基本流程：

1. **初始化**：
   - 选择一个初始样本集，通常通过随机采样、拉丁超立方抽样（LHS）或其他抽样方法在定义的搜索空间内生成。
   - 使用这些初始样本点评估目标函数，获取初始的目标函数值。

2. **构建高斯过程模型**：
   - 利用初始样本点及其对应的目标函数值来训练一个高斯过程模型。这个模型会估计目标函数在任何未评估点的均值和方差。

3. **定义采集函数**：
   - 选择一个采集函数，如期望改进（EI）、高斯过程上置信界限（GP-UCB）或概率改进（PI），来衡量不同候选点的价值。

4. **优化采集函数**：
   - 使用优化算法（如遗传算法、梯度下降或直接搜索）来找到使采集函数最大化的点。这个点是在当前高斯过程模型下最有潜力的点。

5. **评估和更新**：
   - 在采集函数选出的点上评估目标函数，获取新的目标函数值。
   - 将新评估的点及其目标函数值添加到样本集中。
   - 使用更新后的样本集重新训练高斯过程模型。

6. **检查终止条件**：
   - 检查是否达到了预设的迭代次数、是否找到了满意的解或是否超出了预算的评估次数。
   - 如果没有达到终止条件，返回步骤3继续优化采集函数。

7. **输出最优解**：
   - 一旦满足终止条件，输出当前找到的最佳解作为优化结果。

贝叶斯优化的关键在于高斯过程模型和采集函数的使用，它们使得算法能够在探索（寻找新的有潜力的区域）和利用（改进当前最佳解）之间做出平衡。通过这种方式，贝叶斯优化能够有效地搜索复杂的搜索空间，尤其是在高维空间和目标函数评估成本高昂的情况下。

# 采集函数简介

## 什么是采集函数
采集函数在贝叶斯优化中用于确定下一个评估点，其输入和输出如下：

### 输入

1. **当前的高斯过程模型**：这个模型是基于之前评估的样本点（`sample_x`）及其对应的目标函数值（`sample_y`）训练得到的。它包含了高斯过程的参数，如均值函数、协方差函数（核函数）参数、训练数据等。

2. **候选点**：这是尚未评估的点，通常表示为一个向量 `x`，其维度与优化问题中的变量数量相同。

3. **当前最佳目标函数值**：这是迄今为止在优化过程中找到的最佳目标函数值，记为 `f_min`。

### 输出

1. **采集函数值**：对于每个候选点，采集函数计算一个数值，表示该点的“价值”或“吸引力”。这个值是基于当前高斯过程模型的预测均值和标准差计算得到的，用于指导选择下一个评估点。

2. **下一个评估点**：在所有候选点中，采集函数值最高的点被选为下一个评估点。这个点将被用于实际的目标函数评估。

### 具体采集函数的输入输出示例

以期望改进（Expected Improvement, EI）采集函数为例：

**输入**：
- `x`：候选点，一个向量。
- `kriging_model`：当前的高斯过程模型，包含了训练数据和模型参数。
- `f_min`：当前最佳目标函数值。

**输出**：
- `EI(x)`：对于候选点 `x`，计算得到的期望改进值。

在实际的优化过程中，采集函数会被应用于一组候选点（例如，通过拉丁超立方抽样生成的点），然后选择具有最高采集函数值的点作为下一个评估点。

这个过程可以总结为：
1. 使用高斯过程模型预测每个候选点的目标函数值和不确定性（标准差）。
2. 根据这些预测值计算每个候选点的采集函数值。
3. 选择具有最高采集函数值的点作为下一个评估点。

这种机制确保了在优化过程中，我们能够平衡对已知区域的利用和对未知区域的探索。

## 候选点怎么获得

在贝叶斯优化中，候选点的生成是寻找下一个评估点的关键步骤。候选点通常是在定义的搜索空间内随机或有策略地生成的。以下是几种常用的方法来生成候选点：

1. 随机采样
这是最直接的方法，即从搜索空间中随机选择点。这种方法简单，但可能效率较低，因为它不利用已有的信息来指导搜索。
2. 拉丁超立方抽样（Latin Hypercube Sampling, LHS）
拉丁超立方抽样是一种统计方法，用于生成近似均匀分布的样本点。它将每个维度划分为多个区间，并从每个区间中随机选择一个点，确保每个维度的每个区间都被覆盖。这种方法可以更均匀地探索搜索空间，通常用于初始样本点的生成。
3. 基于模型的采样
在贝叶斯优化中，基于当前高斯过程模型的采样是一种常见方法。这可以进一步细分为：
- **基于采集函数的采样**：使用采集函数（如期望改进EI、高斯过程上置信界限GP-UCB等）来评估候选点的价值，并选择最优的候选点。
- **基于概率的采样**：例如，汤普森采样，它从后验分布中直接采样新点。
4. 进化算法
进化算法（如遗传算法）可以用来生成候选点。这些算法通过模拟自然选择的过程，迭代地改进候选解。在每次迭代中，基于当前种群，通过选择、交叉和变异操作生成新的候选点。
5. 网格搜索
在一些简单的问题中，可以使用网格搜索来生成候选点。这种方法在每个维度上定义一个网格，并在网格的每个交点处评估目标函数。这种方法在维度较高时计算成本会急剧增加（维度灾难）。
6. 低差异序列
低差异序列（如Sobol序列、Halton序列）是另一种用于生成候选点的方法。这些序列在高维空间中提供了更好的均匀性和低相关性，适合于高维优化问题。
7. 基于梯度的优化
如果目标函数或采集函数的梯度可以计算，可以使用梯度下降等基于梯度的方法来寻找下一个评估点。

在实际应用中，选择哪种方法来生成候选点取决于具体问题的性质、目标函数的特性以及优化算法的要求。通常，结合多种方法可以提高优化的效率和效果。

# matlab 标准贝叶斯代码解析
给出的m文件中简介如下：

这些代码文件是用于贝叶斯优化（Bayesian Optimization）的MATLAB脚本。贝叶斯优化是一种全局优化算法，它使用贝叶斯统计推断来选择下一个评估点，以最小化目标函数。这种方法特别适用于那些计算成本高昂的黑箱函数，即我们不知道函数的显式形式，只能通过评估来获取函数值的情况。以下是每个文件的解释：

1. **Ackley.m**：定义了Ackley函数，这是一个用于测试优化算法性能的非凸函数。

2. **BO.m**：这是贝叶斯优化的主要脚本。它首先清除变量，清空命令窗口，关闭所有图形界面。然后，它设置了要优化的目标函数（如Rosenbrock函数），变量数量，初始样本数量和最大评估次数。接着，它使用拉丁超立方抽样（lhsdesign）生成初始样本，并评估这些样本的目标函数值。然后，它进入一个循环，在每次迭代中，使用高斯过程（GP）模型来预测下一个评估点（使用优化算法GA），并在该点评估目标函数。这个过程会重复直到达到最大评估次数。

3. **Ellipsoid.m**：定义了Ellipsoid函数，这是另一个用于测试优化算法性能的非凸函数。

4. **GP_predict.m**：这个函数用于根据高斯过程模型进行预测。它接受测试点和模型作为输入，并返回预测值和预测的方差。

5. **GP_train.m**：这个函数用于训练高斯过程模型。它接受样本点和对应的目标函数值，并返回训练好的模型参数。

6. **Griewank.m**：定义了Griewank函数，这是另一个用于测试优化算法性能的非凸函数。

7. **Infill_EI.m**：定义了期望改进（Expected Improvement, EI）准则，这是一种常用的贝叶斯优化中的采集函数（acquisition function），用于选择下一个评估点。

8. **Optimizer_GA.m**：这是一个遗传算法（Genetic Algorithm, GA）的实现，用于在贝叶斯优化中寻找采集函数的最小值，即下一个评估点。

9. **Rosenbrock.m**：定义了Rosenbrock函数，这是一个用于测试优化算法性能的非凸函数。

10. **Test_Function.m**：这个函数根据函数名称和变量数量，返回不同测试函数的上下界。这些边界用于生成初始样本点。

总的来说，这些代码文件共同实现了一个贝叶斯优化流程，其中包括目标函数的定义、初始样本的生成、高斯过程模型的训练和预测、采集函数的计算以及遗传算法的实现。这些组件共同工作，以找到给定目标函数的最小值。


