
# 一、安装spark
## 1、本地模式
安装包链接：[Index of /dist/spark/spark-3.1.1](https://archive.apache.org/dist/spark/spark-3.1.1/)
下载⽂件名：spark-3.1.1-bin-hadoop3.2.tgz 

（1）安装 打开放置压缩包的⽂件夹（可以使⽤共享文件夹传输） 执⾏代码：
```
tar -zxvf /mnt/hgfs/sharevm/spark-3.1.1-bin-hadoop3.2.tgz -C /app/
```

配置环境变量
```
sudo vim /etc/profile
```

添加以下
```
export SPARK_HOME=/app/spark-3.1.1-bin-hadoop3.2
export PATH=$PATH:$SPARK_HOME/bin
```

生效命令
```
source /etc/profile
```

(2）查看

打开hadoop
```
/app/hadoop-3.4.0/sbin/start-all.sh
```

运行代码
```
spark-shell --help
```

![](../../attachments/Pasted%20image%2020241026141219.png)

运行代码
```
spark-shell --version
```
![](../../attachments/Pasted%20image%2020241026141259.png)

运行代码
```
spark-shell --master local[2]
```
![](../../attachments/Pasted%20image%2020241026141350.png)

（3）测试
执⾏代码： 创建input⽂件夹：
```
hadoop fs -mkdir -p /input
```

新建⼀个notes.txt⽂件： 
```
echo "Scala is a powerful language. Scala allows developers to write concise and efficient code. Scala is also versatile and supports both functional and object-oriented programming paradigms." > notes.txt 
```
把notes.txt⽂件放⼊hadoop⽂件夹input中：
```
hadoop fs -put notes.txt /input/notes.txt 
```

查看⽂件是否上传成功（如果有内容展示则步骤正确） 
```
hadoop fs -cat /input/notes.txt
```

![](../../attachments/Pasted%20image%2020241026141814.png)

切换⽂件夹： 
```
cd /app/hadoop-3.4.0/etc/hadoop
```
查看所有⽂件（检查txt⽂件是否存在）：
```
ls
```
![](../../attachments/Pasted%20image%2020241026142217.png)

打开spark
```
spark-shell
```

代码
记得将"hdfs://192.168.95.130:8020“ 改成自己配置的
```
val file = sc.textFile("hdfs://192.168.95.130:8020/input/notes.txt") 

val words = file.flatMap(_.split("\\s+")).map(word => (word, 1)).reduceByKey(_ + _) 

words.collect
```

排错
```
cp /app/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh.template /app/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh
```

```
vi /app/spark-3.1.1-bin-hadoop3.2/conf/spark-env.sh
```

```
export JAVA_HOME=/usr/java/jdk1.8.0_421
```

```
export SPARK_DIST_CLASSPATH=$(/app/hadoop-3.4.0/bin/hadoop classpath)
```

运行结果：
![](../../attachments/Pasted%20image%2020241026145013.png)

## 2.伪分布安装

（1）免密登录
该操作已经于作业1完成，不再重复

（2）伪分布式安装
打开⽂件
```
vim /app/spark-3.1.1-bin-hadoop3.2/sbin/spark-conf.sh
```

添加java环境
```
export JAVA_HOME=/usr/java/jdk1.8.0_421
```

保存，再打开workers⽂件
```
vi /app/spark-3.1.1-bin-hadoop3.2/conf/workers
```

输⼊⾃⼰的主机名，并且保存，执⾏代码，启动spark：
```
/app/spark-3.1.1-bin-hadoop3.2/sbin/start-all.sh
```
![](../../attachments/Pasted%20image%2020241026150549.png)

执⾏代码，查看端⼝情况：
```
netstat -nap | grep java
```

执⾏代码：
```
spark-shell --master spark://server130:7077
```

查看8080端⼝（这⾥可能需要使⽤ip地址）：
```
http://192.168.95.130:8080/
```

![](../../attachments/Pasted%20image%2020241026150920.png)

（3）测试
```
val file = sc.textFile("hdfs://192.168.95.130:8020/input/notes.txt") 

val words= file.flatMap(_.split("\\s+")) 

val kv = words.map((_,1)) 

val worldCount = kv.reduceByKey(_+_) 

worldCount.collect.foreach(kv=>println(kv._1+"\t"+kv._2))
```

运行结果
![](../../attachments/Pasted%20image%2020241026151102.png)

# 使用scale开发spark应用

1 . 安装scala和jdk
安装scala：
[Scala 2.12.0 | The Scala Programming Language](https://www.scala-lang.org/download/2.12.0.html)

[Scala安装教程（最详细教程）-CSDN博客](https://blog.csdn.net/Andy86666/article/details/110138243)

安装好后，正确配置系统环境
![](../../attachments/Pasted%20image%2020241026152829.png)
参考 [window 下 win10 jdk8安装与环境变量的配置（超级详细）_jdk8环境变量配置-CSDN博客](https://blog.csdn.net/weixin_43731532/article/details/112461151) 重新安装jdk

[IDEA中scala安装与配置（详细步骤）_scala idea-CSDN博客](https://blog.csdn.net/qq_46087202/article/details/123711939)

2.开发spark程序
打开源码⽂件，添加框架⽀持，对于⾼版本可能需要在搜索框进⾏搜索“框架⽀持” 运⾏结果：
![](../../attachments/Pasted%20image%2020241026163330.png)
把scala⽂件设置为resource root，运⾏效果：
![](../../attachments/Pasted%20image%2020241026174417.png)

（1）打开scala⽂件下的HelloScala.scala⽂件，按照下述⽅式，运⾏当前⽂件
将代码： package org.hadoop 改为 package org.hadoop.spark 运⾏该⽂件，运⾏效果：
![](../../attachments/Pasted%20image%2020241026174216.png)

使⽤maven打包，使⽤ctrl添加maven⽣命周期
![|500](../../attachments/Pasted%20image%2020241026175710.png)

运⾏成功在控制台(cmd)获取⽣成⽂件的路径，并且运⾏代码
```
java -cp chapter11-1.0.jar;%SCALA_HOME%\lib\* org.hadoop.spark.HelloScala
```
![](../../attachments/Pasted%20image%2020241026183645.png)


(2）打开WordCount.scala，查看14⾏的路径，在该路径下⾯添加a.txt⽂件，并且输⼊内容
```
Scala is a powerful language. Scala allows developers to write concise and efficient code. Scala is also versatile and supports both functional and object-oriented programming paradigms
```

运行效果
![](../../attachments/Pasted%20image%2020241026183002.png)
![|500](../../attachments/Pasted%20image%2020241026183043.png)

# 三 .spark-submit
在虚拟机打开hadoop，在命令⾏中输⼊
```
spark-submit
```
![](../../attachments/Pasted%20image%2020241026183919.png)

在本地windows中，打开WordCount2.scala，使⽤2节最后⾯的maven打包⽅式进⾏打包，并且 把包传到虚拟机，可以新建⼀个⽂件夹，但是后⾯的路径需要跟⽂件的路径⼀致 上传⽂件位置：
此处我使用共享文件夹上传，并使用cp命令拷贝到目标文件夹
```
mkdir /usr/jar/
```

```
cp /mnt/hgfs/sharevm/chapter11-1.0.jar /usr/jar/chapter11-1.0.jar
```

![](../../attachments/Pasted%20image%2020241026184458.png)

运行以下代码
```
spark-submit --master spark://server130:7077 --class org.hadoop.spark.WordCount2 /usr/jar/chapter11-1.0.jar hdfs://server130:8020/test/ hdfs://myserver:8020/out001
```
![](../../attachments/Pasted%20image%2020241026184643.png)

# 四 .DataFrame
1 . 创建createDataFram

如下图创建createDataFram.scala
![](../../attachments/Pasted%20image%2020241026185602.png)

输入代码
```
package com.hollysys.spark  
import java.util  
import org.apache.spark.sql.types._  
import org.apache.spark.sql.{Row, SQLContext, SparkSession}  
object CreateDataFrameTest {  
def main(args: Array[String]): Unit = {  
val spark = SparkSession  
        .builder()  
        .appName(this.getClass.getSimpleName).master("local")  
        .getOrCreate()  
//通过Seq⽣成  
val df = spark.createDataFrame(Seq(  
                ("ming", 20, 15552211521L),  
        ("hong", 19, 13287994007L),  
 ("zhi", 21, 15552211523L)  
         )) toDF("name", "age", "phone")  
 df.show()  
//动态创建schema  
val schema = StructType(List(  
        StructField("name", StringType, true),  
        StructField("age", IntegerType, true),  
        StructField("phone", LongType, true)  
))  
val dataList = new util.ArrayList[Row]()  
        dataList.add(Row("ming",20,15552211521L))  
        dataList.add(Row("hong",19,13287994007L))  
        dataList.add(Row("zhi",21,15552211523L))  
        spark.createDataFrame(dataList,schema).show()  
 }  
         }
```

运行效果
![](../../attachments/Pasted%20image%2020241026185454.png)
![](../../attachments/Pasted%20image%2020241026185518.png)

2 . select和selectExpr⽅法
创建一个selecl类，输入以下代码，运行一个方法时注释另外一部分代码

代码：
```
//selece⽅法  
import org.apache.spark.sql.SparkSession  
object select {  
  def main(args: Array[String]): Unit = {  
    val spark = SparkSession  
      .builder() //创建Spark会话  
      .appName("Spark SQL basic example") //设置会话名称  
      .master("local") //设置本地模式  
      .getOrCreate() //创建会话变量  
    val rdd = spark.sparkContext.parallelize(Array(1,2,3,4))  
    import spark.implicits._  
    val df = rdd.toDF("id")  
    df.select("id").show() //选择“id”列  
  }  
}  
  
  
//selectExpr⽅法  
import org.apache.spark.sql.SparkSession  
object select {  
  def main(args: Array[String]): Unit = {  
    val spark = SparkSession  
      .builder() //创建Spark会话  
      .appName("Spark SQL basic example") //设置会话名称  
      .master("local") //设置本地模式  
      .getOrCreate() //创建会话变量  
    val rdd = spark.sparkContext.parallelize(Array(1,2,3,4))  
    import spark.implicits._  
    val df = rdd.toDF("id")  
    df.selectExpr("id as ID").show() //设置了⼀个别名ID  
  }  
}
```

select方法
![](../../attachments/Pasted%20image%2020241026190015.png)

selectExpr
![](../../attachments/Pasted%20image%2020241026190128.png)

3 . collect⽅法

```
import org.apache.spark.sql.SparkSession  
object collect {  
  def main(args: Array[String]): Unit = {  
    val spark = SparkSession  
      .builder() //创建Spark会话  
      .appName("Spark SQL basic example") //设置会话名称  
      .master("local") //设置本地模式  
      .getOrCreate() //创建会话变量  
    val rdd = spark.sparkContext.parallelize(Array(1,2,3,4))  
    import spark.implicits._  
    val df = rdd.toDF("id")  
    val arr = df.collect()  
    println(arr.mkString("Array(", ", ", ")"))  
  }  
}
```

![](../../attachments/Pasted%20image%2020241026233653.png)

4 . DataFrame计算⾏数count⽅法
```
import org.apache.spark.sql.SparkSession  
object count {  
  def main(args: Array[String]): Unit = {  
    val spark = SparkSession  
      .builder() //创建Spark会话  
      .appName("Spark SQL basic example") //设置会话名称  
      .master("local") //设置本地模式  
      .getOrCreate() //创建会话变量  
    val rdd = spark.sparkContext.parallelize(Array(1,2,3,4))  
    import spark.implicits._  
    val df = rdd.toDF("id")  
    println(df.count()) //计算⾏数  
  }  
}
```

![](../../attachments/Pasted%20image%2020241026233758.png)

5 . 过滤数据的filter⽅法

```
import org.apache.spark.sql.SparkSession  
object fliter {  
  def main(args: Array[String]): Unit = {  
    val spark = SparkSession  
      .builder() //创建Spark会话  
      .appName("Spark SQL basic example") //设置会话名称  
      .master("local") //设置本地模式  
      .getOrCreate() //创建会话变量  
    val rdd = spark.sparkContext.parallelize(Array(1,2,3,4))  
    import spark.implicits._  
    val df = rdd.toDF("id")  
    val df2 = df.filter("id>3")//过滤id列⼤于3的数据（⾏）或 _ >= 3    println(df2.cache().show()) //打印结果  
  }  
}
```

![](../../attachments/Pasted%20image%2020241026233909.png)

6 . 以整体数据为单位操作数据的flatMap⽅法
```
import org.apache.spark.sql.SparkSession  
object flatmap {  
  def main(args: Array[String]): Unit = {  
    val spark = SparkSession  
      .builder() //创建Spark会话  
      .appName("Spark SQL basic example") //设置会话名称  
      .master("local") //设置本地模式  
      .getOrCreate() //创建会话变量  
    val rdd = spark.sparkContext.parallelize(Seq("hello!spark", "hello!hadoop"))  
    import spark.implicits._  
    val df = rdd.toDF("id")  
    val x = df.flatMap(x => x.toString().split("!")).collect()  
    println(x.mkString("Array(", ", ", ")"))  
  }  
}
```

![](../../attachments/Pasted%20image%2020241026234030.png)

7 . 分组数据的groupBy和agg⽅法

```
import org.apache.spark.sql.SparkSession  
object GroupByExample {  
  def main(args: Array[String]): Unit = {  
    val spark = SparkSession  
      .builder()  
      .appName("Spark SQL basic example")  
      .master("local")  
      .getOrCreate()  
    // 导⼊隐式转换，获取默认的编码器  
    import spark.implicits._  
    // 创建 JSON 字符串数组的 Dataset    val jsonDataSet = spark.createDataset(Array(  
      "{\"name\":\"ming\",\"age\":20,\"phone\":15552211521}",  
      "{\"name\":\"hong\", \"age\":19,\"phone\":13287994007}",  
      "{\"name\":\"zhi\", \"age\":21,\"phone\":15552211523}"  
    ))  
    // 将 JSON 数据集转换为 DataFrame    val jsonDataSetDf = spark.read.json(jsonDataSet)  
    // 显示 DataFrame 的内容  
    jsonDataSetDf.groupBy("name").agg("age" -> "count").show()  
    spark.stop()  
  }  
}
```

![](../../attachments/Pasted%20image%2020241026234136.png)

8 . 删除数据集中某列的drop⽅法

```
import org.apache.spark.sql.SparkSession  
object GroupByExample2 {  
  def main(args: Array[String]): Unit = {  
    val spark = SparkSession  
      .builder()  
      .appName("Spark SQL basic example")  
      .master("local")  
      .getOrCreate()  
    // 导⼊隐式转换，获取默认的编码器  
    import spark.implicits._  
    // 创建 JSON 字符串数组的 Dataset    val jsonDataSet = spark.createDataset(Array(  
      "{\"name\":\"ming\",\"age\":20,\"phone\":15552211521}",  
      "{\"name\":\"hong\", \"age\":19,\"phone\":13287994007}",  
      "{\"name\":\"zhi\", \"age\":21,\"phone\":15552211523}"  
    ))  
    // 将 JSON 数据集转换为 DataFrame    val jsonDataSetDf = spark.read.json(jsonDataSet)  
    // 显示 DataFrame 的内容  
    jsonDataSetDf.drop("age").show()  
    //删除age列  
    spark.stop()  
  }  
}
```

![](../../attachments/Pasted%20image%2020241026234331.png)

# 五 . Spark SQL

以下非scala文件在虚拟机中打开spark执行

1 . 示例
(1） 给DataFrame设置别名（⼀⾏代码⼀⾏代码运⾏）

```
val rdd=sc.makeRDD(Seq(("Jack",24),("Mary",34)));  
val df1 = rdd.toDF();  
val df2 = rdd.toDF("name","age");  
df1.show();  
df2.show();
```

![](../../attachments/Pasted%20image%2020241026234600.png)

(2）使⽤SqlContext执⾏SQL语句 （继续上⾯的代码运⾏）

```
df2.createOrReplaceTempView("person");  
val sqlContext = spark.sqlContext  
sqlContext.sql("select * from person").show();
```

![](../../attachments/Pasted%20image%2020241026234708.png)

(3)Scala代码将RDD转成Bean

在D:/a路径下⾯，创建stud.txt⽂件，⽂件内容：

```
4 Jack 34 男  
1 Mike 23 ⼥  
2 刘⻓友 45 男  
3 雪丽 27 ⼥
```

在pom.xml⽂件中添加依赖
```
<dependency>  
  <groupId>org.apache.spark</groupId>  
  <artifactId>spark-sql_2.12</artifactId> <version>3.1.1</version>  
</dependency>
```

创建scala class 复制以下代码运行
```
package org.hadoop.spark  
import org.apache.spark.rdd.RDD  
import org.apache.spark.{SparkConf, SparkContext}  
import org.apache.spark.sql.SparkSession  
object RddToBean1 {  
  def main(args: Array[String]): Unit = {  
    val conf: SparkConf = new SparkConf();  
    conf.setMaster("local[*]");  
    conf.setAppName("RDDToBean");  
    val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate();  
    val sc: SparkContext = spark.sparkContext;  
    //读取⽂件  
    import spark.implicits._;  
    //步2：读取⽂件  
    val rdd: RDD[String] = sc.textFile("file:///D:/a/stud.txt");  
    //步3：第⼀次使⽤map对每⼀⾏进⾏split，第⼆次使⽤map将数据封装到Bean中,最后使⽤t oDF转换成DataFrame  
    val df = rdd.map(_.split("\\s+")).map(arr => {  
      Stud(arr(0).toInt, arr(1), arr(2).toInt, arr(3));  
    }).toDF();  
    //步4：显示或是保存数据  
    df.show();  
    spark.close();  
  }  
  /** 步1：声明JavaBean，并直接声明主构造⽅法 * */  
  case class Stud(id: Int, name: String, age: Int, sex: String) {  
    /** 声明⽆参数的构造，调⽤主构造⽅法 * */  
    def this() = this(-1, null, -1, null);  
  }  
}
```

运行结果
![](../../attachments/Pasted%20image%2020241026235144.png)

(4）WordCount示例

直接使⽤groupBy进⾏count计算
```
val rdd = sc.textFile("hdfs://192.168.95.130:8020/input/notes.txt")  
val df3 = rdd.flatMap(_.split("\\s+")).toDF("str");  
df3.groupBy("str").count().show();//运⾏结果⻅1图
```

![](../../attachments/Pasted%20image%2020241026235434.png)

指定排序规则
```
df3.groupBy("str").count().sort("str").show();//对字符进⾏排序
```
![](../../attachments/Pasted%20image%2020241026235636.png)

直接使⽤SQL语句
```
df3.createOrReplaceTempView("words"); 
spark.sql("select count(str),str from words group by str").show
```

![](../../attachments/Pasted%20image%2020241026235808.png)

(5)Scala代码示例
运⾏SparkSQL.scala
![](../../attachments/Pasted%20image%2020241026235958.png)

运⾏SparkSQL2.scala
![](../../attachments/Pasted%20image%2020241027000139.png)

2 . Read和Write 可以读写的数据类型
```
csv format jdbc json load option options 
orc parquet schema table text textFile
```

六 . Spark Streaming
1 .示例
(1) 将依赖⽂件加⼊pom.xml
```
<dependency>  
  <groupId>org.apache.spark</groupId>  
  <artifactId>spark-streaming_2.12</artifactId>  
  <version>3.1.1</version>  
</dependency>
```

(2) 打开虚拟机,配置镜像⽂件,如果配置过,请忽略此步骤
作业1里已经配置过

(3) 安装nc软件
```
sudo yum install -y nc
```
![](../../attachments/Pasted%20image%2020241027000719.png)

(4) 在本地idea项⽬中,运⾏ Streaming.java 

(5)在虚拟机启动⼀个nc端⼝
```
nc -lk 9999
```

再在控制台输⼊
```
Jack Mary
```

![](../../attachments/Pasted%20image%2020241027001038.png)

(7) 再在12代码后(即val的声明后⾯)添加代码:
```
lines.flatMap(_.split("\\s+")).map((_, 1)).reduceByKey(_+_).print();
```

![](../../attachments/Pasted%20image%2020241027205232.png)

(8)IDEA打开Streaming2.scala,设置⾃⼰的hdfs地址,可以使⽤以下代码查看在虚拟机,并且更改IDEA第7 12⾏的 代码
```
hdfs getconf -confKey fs.defaultFS
```
![](../../attachments/Pasted%20image%2020241027205434.png)

第10⾏修改⾃⼰的主机名,通过以下代码在虚拟机查看
```
hostname
```
![](../../attachments/Pasted%20image%2020241027205500.png)

使⽤Maven打包项⽬，使用共享文件夹操作，并移动到/usr/jar
```
cp /mnt/hgfs/sharevm/chapter11-1.0.jar /usr/jar/chapter11-1.0.jar
```

(9) 使⽤nc创建端⼝（可以使⽤另外⼀个控制台使⽤nc）
```
nc -lk 4444
```

```
cd /app/spark-3.1.1-bin-hadoop3.2/sbin 
./start-master.sh 
./start-worker.sh spark://192.168.95.130:7077
```

![](../../attachments/Pasted%20image%2020241027210159.png)

运⾏打包的项⽬，根据⾃⼰的位置和端⼝进⾏修改
```
spark-submit --master spark://server130:7077 --class org.hadoop.spark.Streaming2 /usr/jar/chapter11-1.0.jar hdfs://192.168.95.130:8020/test/ hdfs://192.168.95.130:8020/out001
```

![](../../attachments/Pasted%20image%2020241027210748.png)

![](../../attachments/Pasted%20image%2020241027210757.png)
![](../../attachments/Pasted%20image%2020241027210808.png)

2 .DStream
虚拟机打开控制台，输⼊代码后，输⼊数据
```
nc -lk 9999
```

打开DStreaming.scala，修改19⾏的ip地址改成⾃⼰的，并且运⾏
![](../../attachments/Pasted%20image%2020241027211003.png)
![](../../attachments/Pasted%20image%2020241027211052.png)

3 . FileStream
（1）打开FileStreaming.scala⽂件，使⽤Maven打包，并且复制到app⽂件夹下 运⾏代码，
需要修改⾃⼰的主机名
```
cp /mnt/hgfs/sharevm/chapter11-1.0.jar /app/chapter11-1.0.jar
```

```
spark-submit --master spark://server130:7077 --class org.hadoop.spark.FileStreaming /app/chapter11-1.0.jar
```

使⽤winscp，往app⽂件⽬录下⾯添加⽂件b.txt，内容如下后
```
my name is aday
```

```
cp /mnt/hgfs/sharevm/b.txt /home/hadoop/1/1.txt
```

![](../../attachments/Pasted%20image%2020241027213100.png)

（2） 打开FileStreaming2.scala，按照上进⾏打包，并且复制到app⽂件夹下 
```
cp /mnt/hgfs/sharevm/chapter11-1.0.jar /app/chapter11-1.0.jar
```
⼀个控制台
```
spark-submit --master spark://server130:7077 --class org.hadoop.spark.FIleStreaming2 /app/chapter11-1.0.jar hdfs://192.168.95.130:8020/test/
```


在虚拟机运⾏，创建hdfs⽂件夹，⼀个控制台
```
hadoop fs -mkdir -p /test 
echo "This is a test file." > /app/testfile.txt 
hadoop fs -put /app/testfile.txt /test/
```
![](../../attachments/Pasted%20image%2020241027213422.png)


![](../../attachments/Pasted%20image%2020241027215219.png)

![](../../attachments/Pasted%20image%2020241027215236.png)

4 . 窗⼝函数
(1) 在虚拟机打开9999端⼝并且监控
```
nc -lk 9999
```

(2) 打开⽂件WindowFun.scal,修改主机名为⾃⼰的,并且运⾏

(3)往虚拟机nc中输⼊数据
```

hello world 
spark streaming 
window function

```
![](../../attachments/Pasted%20image%2020241027220208.png)

5 . updateStateByKey
(1) 运⾏ForUpdateStateByKey.java,路径如下:

```
src/main/java/org/hadoop/spark/ForUpdateStateByKey.java
```

(2) 再运⾏该⽬录下的 UpdateStateByKeyClient.scala，将ip改成localhost

![](../../attachments/Pasted%20image%2020241027221510.png)
![](../../attachments/Pasted%20image%2020241027221440.png)
