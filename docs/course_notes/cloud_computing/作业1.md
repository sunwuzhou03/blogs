# 作业1

## 参考链接：
[云计算作业一-CSDN博客](https://blog.csdn.net/qq_53877854/article/details/142412784)

## 0.前置准备
### CentOS7安装教程
[CentOS7(Linux)详细安装教程（手把手图文详解版）-CSDN博客](https://blog.csdn.net/qq_57492774/article/details/131772646)
[在VMware中安装CentOS7（超详细的图文教程）_在vmware上安装centos-CSDN博客](https://blog.csdn.net/qq_45743985/article/details/121152504)

[centos-7.9.2009-isos-x86_64安装包下载_开源镜像站-阿里云 (aliyun.com)](https://mirrors.aliyun.com/centos/7.9.2009/isos/x86_64/)
选择torrent文件下载再使用迅雷打开，这样下载速度最快
![](../../attachments/Pasted%20image%2020241006121831.png)

### 创建管理员用户
![](../../attachments/Pasted%20image%2020241006162734.png)
### yum 换源
[云服务器 CentOS 操作系统切换 YUM 源-运维指南-文档中心-腾讯云 (tencent.com)](https://cloud.tencent.com/document/product/213/52559)

![](../../attachments/Pasted%20image%2020241006173045.png)

### 命令行粘贴复制
[【踩坑 】如何将主机端内容粘贴到 VMware 虚拟机里面的 CentOSminimal 或者 Anolis minimal 迷你版本？(VMware | minimal | Centos ）)_怎么将主机文字复制到虚拟机上-CSDN博客](https://blog.csdn.net/weixin_49486457/article/details/130415638)

### 命令行安装图形界面
[VMware Centos 7操作系统安装图形化界面GNOME Desktop，从命令行模式安装图形化界面，转换到图形化界面（适用于：已经安装好centos操作系统，由于在之前没有选择安装图形化界面）_centos7安装图形化界面命令-CSDN博客](https://blog.csdn.net/qq_35353972/article/details/142586250)

### 图形界面以及命令行界面相互转换
[两种CentOS7命令行模式和图形模式切换方法（简单常用）_centos7图形化界面和命令行的切换-CSDN博客](https://blog.csdn.net/qq_22903531/article/details/113931829)

### 虚拟机卡顿
[VMware虚拟机经常性卡死，打开运行一段时间后卡死，CPU占比增至100% - MXT16 - 博客园 (cnblogs.com)](https://www.cnblogs.com/meng-xiang-tao-1999/p/17335673.html)

记住不要取消勾选**虚拟化平台以及windows虚拟化监控**
### 解决图形界面粘贴复制以及主机端到客户端的复制粘贴

安装vmware-tools
[CentOS 7命令行模式安装VMware Tools 详解_centos7命令安装vmware-tools-CSDN博客](https://blog.csdn.net/longzhoufeng/article/details/84067781)

设置虚拟机隔离选项
[虚拟机VMware客户机隔离灰色如何解决||实现本机复制粘贴到虚拟机_虚拟机客户机隔离灰色怎么解决-CSDN博客](https://blog.csdn.net/qq_45788060/article/details/134575588)

[【踩坑 】如何将主机端内容粘贴到 VMware 虚拟机里面的 CentOSminimal 或者 Anolis minimal 迷你版本？(VMware | minimal | Centos ）)_怎么将主机文字复制到虚拟机上-CSDN博客](https://blog.csdn.net/weixin_49486457/article/details/130415638)


### 静态IP设置

#### ip addr 查看ip地址
```
ip addr
```
![](../../attachments/Pasted%20image%2020241007214442.png)

#### ifconfig查看ip
[CentOS7：ifconfig command not found解决-CSDN博客](https://blog.csdn.net/dandelion_drq/article/details/53503487)
```
ifconfig
```
![](../../attachments/Pasted%20image%2020241007220330.png)

#### 改写ifcfg-ens33

#### 安装vim
```
yum -y install vim-enhanced
```

```
vim /etc/sysconfig/network-scripts/ifcfg-ens33
```

**需要修改 BOOTPROTO=static**
#### 需要添加的东西
```
PEERDNS=yes
PEERROUTES=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes

IPADDR=192.168.95.130
GATEWAY=192.168.95.1
NETMASK=255.255.255.0
DNS1=8.8.8.8
DNS2=8.8.4.4
```

### 0.2 Linux统一设置
步骤01 配置主机名称
```
hostnamectl set-hostname server130
```

步骤02 修改host文件

```
sudo vim /etc/hosts
```

```
192.168.95.130  server130
```

步骤03 关闭并禁用防火墙
```
systemctl stop firewalld
systemctl disable firewalld
```
![](../../attachments/Pasted%20image%2020241007222245.png)

步骤04 禁用SELinux，需要重新启动
vim 添加SELINUX=disabled
```
vim /etc/selinux/config
SELINUX=disabled
```
![](../../attachments/Pasted%20image%2020241007222412.png)

步骤05 在/usr/java目录下，安装JDK1.8.x。
[Linux版本JDK1.8下载与安装_jdk-8u281-linux-x64.rpm下载-CSDN博客](https://blog.csdn.net/qq_37975919/article/details/115262608)

[在oracle下载jdk-8u162-linux-x64.tar.gz显示400 Bad Request Request Header Or Cookie Too Large_oracle下载出现400-CSDN博客](https://blog.csdn.net/m0_73804746/article/details/142713921)


主机端共享文件进行上传 [主机与VMware虚拟机共享文件夹：解决虚拟机找不到共享文件夹问题 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/650638983)
在主机和客户端建立好共享文件后，生效的命令


```
sudo mount -t fuse.vmhgfs-fuse .host:/ /mnt/hgfs -o allow_other
```

步骤06 配置JAVA_HOME环境变量。
```
vim /etc/profile
```

在profile文件最后，添加以下配置：
```
export JAVA_HOME=/usr/java/jdk1.8.0_421
export PATH=.:$PATH:$JAVA_HOME/bin
```

![](../../attachments/Pasted%20image%2020241009201850.png)

让环境变量生效：
```
source /etc/profile
```

检查Java版本：
```
java -version
```

![](../../attachments/Pasted%20image%2020241009201930.png)

## 1. Hadoop安装配置
### 1.1 环境准备
[centos 切换用户 - CSDN文库](https://wenku.csdn.net/answer/1b1009b95ae245fa808422a6d53be2d8)

步骤01 关闭防火墙。
```
sudo firewall-cmd --state
```
![](../../attachments/Pasted%20image%2020241008220330.png)

running表示防火墙正在运行。以下命令用于停止和禁用防火墙：
```
systemctl stop firewalld.service
systemctl disable firewalld.service
```

 步骤02 配置免密码登录。

配置免密码登录的主要目的，就是在使用Hadoop脚本启动Hadoop的守护进程时，不需要再提示用户输入密码。SSH免密码登录的主要实现机制，就是在本地生成一个公钥，然后将公钥配置到需要被免密码登录的主机上，登录时自己持有私钥与公钥进行匹配，如果匹配成功，则登录成功，否则登录失败。

可以使用ssh-keygen命令生成公钥和私钥文件，并将公钥文件复制到被SSH登录的主机上。以下是ssh-keygen命令，输入后直接按两次回车即可生成公钥和私钥文件：
```
ssh-keygen -t rsa
```

![](../../attachments/Pasted%20image%2020241008220730.png)

如上面所说，生成的公钥和私钥文件将被放到~/.ssh/目录下。其中id_rsa文件为私钥文件，rd_rsa.pub为公钥文件。现在我们再使用ssh-copy-id将公钥文件发送到目标主机。由于登录的是本机，所以直接输入本机名即可：

```
ssh-copy-id server130
```
![](../../attachments/Pasted%20image%2020241008221030.png)

此命令执行以后，会在~/.ssh目录下多出一个用于认证的文件，其中保存了某个主机可以登录的公钥信息，这个文件为~/.ssh/authorized_keys。

现在再使用ssh server201命令登录本机，将会发现不用再输入密码，即可以直接登录成功。
```
ssh server130
```
![](../../attachments/Pasted%20image%2020241008221429.png)

### 1.2 Hadoop伪分布式安装
在安装之前，请确定已经安装了JDK1.8，并正确配置了JAVA_HOME、PATH环境变量。

在磁盘根目录下，创建一个app目录，并授权给hadoop用户。然后将会把Hadoop安装到此目录下。先切换到根目录下：
```
cd /
```
添加sudo前缀使用mkdir创建/app目录：
```
sudo mkdir /app
```
将此目录的所有权授予给hadoop用户和hadoop组：
```
sudo chown hadoop:hadoop /app
```
切换进入/app目录：
```
cd /app/
```
使用ll -d命令查看本目录的详细信息，可看到此目录已经属于hadoop用户：
```
ll -d
```
![](../../attachments/Pasted%20image%2020241008222020.png)

下载hadoop：[Index of /apache/hadoop/common/stable (tsinghua.edu.cn)](https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/stable/ "Index of /apache/hadoop/common/stable (tsinghua.edu.cn)")

将Hadoop的压缩包通过共享文件夹上传到虚拟机客户端，再移动到/app目录下，并解压到此文件中。可用ll命令查看本目录是否上传成功。

我创建的分享文件夹名为sharevm
```
cp /mnt/hgfs/sharevm/hadoop-3.4.0.tar.gz /app/
```

```
tar -zxvf hadoop-3.4.0.tar.gz
```
![](../../attachments/Pasted%20image%2020241008222750.png)

以下开始配置Hadoop。Hadoop的所有配置文件都在hadoop-3.4.0/etc/hadoop目录下。首先切换到此目录下，然后开始配置：
```
cd /app/hadoop-3.4.0/etc/hadoop/
```
步骤01 配置hadoop-env.sh文件。

hadoop-env.sh文件是Hadoop的环境文件，在此文件中需要配置JAVA_HOME变量。在此文件的最后一行输入以下配置，然后按Esc键，再输入:wq保存退出即可：

```
vim hadoop-env.sh
```
按下i进入插入模式，在最后一行插入以下
```
export JAVA_HOME=/usr/java/jdk1.8.0_421
```
![](../../attachments/Pasted%20image%2020241009202356.png)

步骤02 配置core-site.xml文件。core-site.xml文件是HDFS的核心配置文件，用于配置HDFS的协议、端口号和地址。注意：Hadoop 3.0以后HDFS的端口号建议为8020，但如果查看Hadoop的官网示例，依然延续使用的是Hadoop 2之前的端口9000，以下配置我们将使用8020端口，只要保证配置的端口没有被占用即可。配置时，需要注意大小写。使用vim打开core-site.xml文件，进入编辑模式：
```
vim core-site.xml
```
在\<configuration>\</configuration>两个标签之间输入以下内容：（注意修改server130，130是本人的命名）
```XML
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://server130:8020</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/datas/hadoop</value>
    </property>
```

![](../../attachments/Pasted%20image%2020241008223437.png)

配置说明：● fs.defaultFS：用于配置HDFS的主协议，默认为file:///。● hadoop.tmp.dir：用于指定NameNode日志及数据的存储目录，默认为/tmp。

步骤03 配置hdfs-site.xml文件。hdfs-site.xml文件用于配置HDFS的存储信息。使用vim打开hdfs-site.xml文件，并在<configuration></configuration>标签中输入以下内容：
```
vim hdfs-site.xml
```

```XML
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
```

![](../../attachments/Pasted%20image%2020241008223627.png)
配置说明：● dfs.replication：用于指定文件块的副本数量。HDFS特别适合于存储大文件，它会将大文件切分成每128MB一块，存储到不同的DataNode节点上，且默认会每一块备份2份，共3份，即此配置的默认值为3，最大为512。由于我们只有一个DataNode，所以这儿将文件副本数量修改为1。● dfs.permissions.enabled：访问时，是否检查安全，默认为true。为了方便访问，暂时把它修改为false。

步骤04 配置mapred-site.xml文件。

通过名称可见，此文件是用于配置MapReduce的配置文件。通过vim打开此文件，并在\<configuration>\</configurati>标签中输入以下配置：
```
vim mapred-site.xml
```

```XML
    <property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
    </property>
```
![](../../attachments/Pasted%20image%2020241008224203.png)
配置说明：● mapreduce.framework.name：用于指定调试方式。这里指定使用YARN作为任务调用方式。

步骤05 配置yarn-site.xml文件。由于上面指定了使用YARN作为任务调度，所以这里需要配置YARN的配置信息，同样，使用vim编辑yarn-site.xml文件，并在\<configuration>\</configuration>标签中输入以下内容：

```
vim yarn-site.xml
```

```XML
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>server130</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
```
![](../../attachments/Pasted%20image%2020241008224431.png)

通过hadoop classpath命令获取所有classpath的目录，然后配置到上述文件中。由于没有配置Hadoop的环境变量，所以这里需要输入完整的Hadoop运行目录，命令如下：

```
/app/hadoop-3.4.0/bin/hadoop classpath
```

```
/app/hadoop-3.4.0/bin/hadoop classpath > hadoop_classpath.txt
```

复制到共享文件夹再主机进行粘贴，然后在虚拟机编辑中选择粘贴到文件中
```
cp ./hadoop_classpath.txt /mnt/hgfs/sharevm
```

![](../../attachments/Pasted%20image%2020241008230528.png)

```
/app/hadoop-3.4.0/etc/hadoop:/app/hadoop-3.4.0/share/hadoop/common/lib/*:/app/hadoop-3.4.0/share/hadoop/common/*:/app/hadoop-3.4.0/share/hadoop/hdfs:/app/hadoop-3.4.0/share/hadoop/hdfs/lib/*:/app/hadoop-3.4.0/share/hadoop/hdfs/*:/app/hadoop-3.4.0/share/hadoop/mapreduce/*:/app/hadoop-3.4.0/share/hadoop/yarn:/app/hadoop-3.4.0/share/hadoop/yarn/lib/*:/app/hadoop-3.4.0/share/hadoop/yarn/*
```

然后将上述的信息复制一下，并配置到yarn-site.xml文件中：
```
vim yarn-site.xml
```

```XML
    <property>
        <name>yarn.application.classpath</name>
        <value>
/app/hadoop-3.4.0/etc/hadoop:/app/hadoop-3.4.0/share/hadoop/common/lib/*:/app/hadoop-3.4.0/share/hadoop/common/*:/app/hadoop-3.4.0/share/hadoop/hdfs:/app/hadoop-3.4.0/share/hadoop/hdfs/lib/*:/app/hadoop-3.4.0/share/hadoop/hdfs/*:/app/hadoop-3.4.0/share/hadoop/mapreduce/*:/app/hadoop-3.4.0/share/hadoop/yarn:/app/hadoop-3.4.0/share/hadoop/yarn/lib/*:/app/hadoop-3.4.0/share/hadoop/yarn/*
        </value>
    </property>
```

![](../../attachments/Pasted%20image%2020241008231253.png)

配置说明：

● yarn.resourcemanager.hostname：用于指定ResourceManger的运行主机，默认为0.0.0.0，即本机。

● yarn.nodemanager.aux-services：用于指定执行计算的方式为mapreduce_shuffle。

● yarn.application.classpath：用于指定运算时的类加载目录。

步骤06 配置workers文件。

这个文件在之前的版本叫作slaves，但功能一样。主要用于在启动时启动DataNode和NodeManager。编辑workers文件，并输入本地名称：

```
vim workers
```

```
server130
```

![](../../attachments/Pasted%20image%2020241008231604.png)

删除localhost 直接加入server130

步骤07 配置Hadoop环境变量。

编辑/etc/profile文件：
```
sudo vim /etc/profile
```

```
export HADOOP_HOME=/app/hadoop-3.4.0

export PATH=$PATH:$HADOOP_HOME/bin
```

![](../../attachments/Pasted%20image%2020241008231911.png)

使用source命令，让环境变量生效：
```
source /etc/profile
```

然后使用hdfs version查看命令环境变量是否生效，如果配置成功，则会显示Hadoop的版本：
```
hdfs version
```
![](../../attachments/Pasted%20image%2020241008232040.png)

步骤08 初始化Hadoop的文件系统。

Hadoop在使用之前，必须先初始化HDFS文件系统，初始化的文件系统将会生成在hadoop.tmp.dir配置的目录下，即上面配置的/app/datas/hadoop目录下。

**注意，如果使用非java.1.8.x 会报security相关错误**
```
hdfs namenode -format
```

如果你想要将 `hdfs namenode -format` 命令的输出重定向到一个日志文件中，以便之后查看，你可以使用以下命令：

切换到root
```
su root
```

初始化，并将日志输入到txt文件中
```
hdfs namenode -format > log.txt 2>&1
```

复制一份到sharevm文件夹中，便于查找信息
```
cp ./log.txt /mnt/hgfs/sharevm/
```

![](../../attachments/Pasted%20image%2020241009204154.png)

步骤09 启动HDFS和YARN。

启动和停止HDFS及YARN的脚本在$HADOOP_HOME/sbin目录下。其中start-dfs.sh为启动HDFS的脚本，start-yarn.sh为启动ResourceManager的脚本。以下命令分别启动HDFS和YARN：

```
/app/hadoop-3.4.0/sbin/start-dfs.sh
```


**排错：**
[Hadoop集群start-dfs.sh错误解决方式-CSDN博客](https://blog.csdn.net/qq_43659234/article/details/108670050)

```
/app/hadoop-3.4.0/sbin/start-dfs.sh > log.txt 2>&1
cp ./log.txt /mnt/hgfs/sharevm/
```

找到sbin文件夹
```
cd /app/hadoop-3.4.0/sbin
```


启动 start-yarn.sh
```
/app/hadoop-3.4.0/sbin/start-yarn.sh
```

启动完成以后，通过jps来查看Java进程快照，会发现有5个进程正在运行：
```
jps
```

![](../../attachments/Pasted%20image%2020241009211124.png)

其中：NameNode、SecondaryNameNode、DataNode是通过start-dfs.sh脚本启动的。ResourceManager和NodeManager是通过start-yarn.sh脚本启动的。启动成功以后，也可以通过

http://server130:9870 查看NameNode的信息，如图所示。

![](../../attachments/Pasted%20image%2020241009211424.png)

http://server130:8088 页面查看MapReduce的信息，如图所示。


修改yarn-site.xml后运行stop-dfs.sh 以及 stop-yarn.sh 再 start-dfs.sh 和 start-yarn.sh

![](../../attachments/Pasted%20image%2020241009212154.png)

步骤10 关闭。

关闭HDFS的YARN可以分别执行stop-dfs.sh和stop-yarn.sh脚本：

```
/app/hadoop-3.4.0/sbin/stop-yarn.sh
```

```
/app/hadoop-3.4.0/sbin/stop-dfs.sh
```

至此，Hadoop单机即伪分布式模式安装和配置成功。

### 1.3 Hadoop集群安装
[Hadoop分布式集群配置_hadoop分配集群-CSDN博客](https://blog.csdn.net/qq_53877854/article/details/142580256)

Hadoop分布式集群配置
在Hadoop的集群中，有一个NameNode，一个ResourceManager。在高可靠的集群环境中，可以拥有两个NameNode和两个ResourceManager；在Hadoop 3版本以后，同一个NameService可以拥有3个NameNode。由于NameNode和ResourceManager是两个主要的服务，建议将它们部署到不同的服务器上。

步骤01 准备工作。

[centos7配置静态ip不生效_centos7设置静态ip不生效-CSDN博客](https://blog.csdn.net/weixin_37569048/article/details/96852643)

本文选择直接克隆实例，如下图所示
![](../../attachments/Pasted%20image%2020241009214840.png)

配置不同主机的静态ip，通过以下命令修改IPADDR和GATEWAY
```
vim /etc/sysconfig/network-scripts/ifcfg-ens33
```

所有主机 修改名字
```
hostnamectl set-hostname server131

```

```
hostnamectl set-hostname server132
```

修改host
```
sudo vim /etc/hosts
```

```
192.168.95.130  server130
192.168.95.131  server131
192.168.95.132  server132
```

关闭所有主机上的防火墙，使用以下命令：
```
systemctl stop firewalld

systemctl disable firewalld
```

生效命令
```
systemctl restart network
```

在主节点（即执行start-dfs.sh和start-yarn.sh的主机）上向所有其他主机做SSH免密码登录。
```
ssh-copy-id root@server130
ssh-copy-id root@server131
ssh-copy-id root@server132
```

[/usr/bin/ssh-copy-id: ERROR: Host key verification failed._ssh-copy-id失败host key verification failed-CSDN博客](https://blog.csdn.net/Moveslow/article/details/141386300)

服务器配置内存部分由于个人电脑原因缩减到 4+1+1，cpu 核数缩减到 2+1+1


配置workers配置文件：workers配置文件用于配置执行DataNode和NodeManager的节点。
```
cd /app/hadoop-3.4.0/etc/hadoop/
```

```
vim workers
```

步骤03 使用scp将Hadoop分发到其他主机。

由于scp会在网络上传递文件，而hadoop/share/doc目录下都是文档，没有必要进行复制，所以可以删除这个目录。删除doc目录：

```
rm -rf /app/hadoop-3.4.0/share/doc
```

```
scp -r /app/hadoop-3.4.0 server131:/app/
```

```
scp -r /app/hadoop-3.4.0 server132:/app/
```

**由于是克隆，这里不需要分发**

步骤04 在server101上格式化NameNode。

首先需要在server101上配置Hadoop的环境变量。

这里配置也不需要，因为单机版本配置了相同的部分。

在server101上执行namenode初始化命令：初始化尝试一下
```
hdfs namenode -format > log.txt 2>&1
```

![](../../attachments/Pasted%20image%2020241010165511.png)


步骤05 启动HDFS和YARN。

在server101上执行启动工作，由于配置了集群，此启动过程会以SSH方式登录其他两台主机，并分别启动DataNode和NodeManager。

```
/app/hadoop-3.4.0/sbin/start-all.sh
```

关闭
```
/app/hadoop-3.4.0/sbin/stop-all.sh
```

stop-all.sh 出现nodemanage did stop, please try to use kill -9，排错
参考链接：[【Hadoop】集群搭建实战：超详细保姆级教程_hadoop集群-CSDN博客](https://blog.csdn.net/weixin_53269650/article/details/141848588)

```
vim /etc/profile
```

```
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```


Datanode 没启动
[hadoop平台datanode节点的默认存放位置_mob649e816138f5的技术博客_51CTO博客](https://blog.51cto.com/u_16175487/11635065)

先删除文件，这个路径是core-site.xml配置的路径
```
rm -rf /opt/datas/hadoop/
```
后重新初始化
```
hdfs namenode -format
```

最终效果
![](../../attachments/Pasted%20image%2020241010183127.png)

发现hadoop访问8088端口显示只有一个节点，我检查yarn-site.xml发现配置错误，有部分写成server201，应该是server130

![](../../attachments/Pasted%20image%2020241010192233.png)

最后，建议执行MapReduce测试一下集群，比如执行以下wordcount示例，如果可以顺序执行完成，则说明整个集群的配置都是正确的。
```
yarn jar hadoop-mapreduce-examples-3.2.2.jar wordcount /test/ /out002
```

[运行Hadoop自带的MapReduce WordCount单词统计程序_mapreduce程序设计基础 1运行hadoop自带的mapreduce程序:wordcount.-CSDN博客](https://blog.csdn.net/weixin_43207025/article/details/100525447)

```
cd /app/hadoop-3.4.0/share/hadoop/mapreduce/
```

```
yarn jar hadoop-mapreduce-examples-3.4.0.jar wordcount /test/ /out002
```
该命令见下面实验

## 2. HDFS实验，包括Shell命令操作和Java接口访问

### 2.1 HDFS操作命令

hdfs命令位于$HADOOP_HOME/bin目录下。由于已经配置了HADOOP_HOME和PATH的环境变量，所以此命令可以在任意目录下执行。可以通过直接输入hdfs命令，查看它的使用帮助：

```
    $ hdfs
    Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND
           where COMMAND is one of:
    dfs    run a filesystem command on the file systems supported in Hadoop.
    classpath            prints the classpath
    namenode -format     format the DFS filesystem
    secondarynamenode    run the DFS secondary namenode
    namenode             run the DFS namenode
    journalnode          run the DFS journalnode
    zkfc                 run the ZK Failover Controller daemon
    datanode             run a DFS datanode
    debug                run a Debug Admin to execute HDFS debug commands
    dfsadmin             run a DFS admin client
    haadmin              run a DFS HA admin client
    fsck                 run a DFS filesystem checking utility
    balancer             run a cluster balancing utility
    jmxget               get JMX exported values from NameNode or DataNode.
    mover                run a utility to move block replicas across
                           storage types
    oiv                  apply the offline fsimage viewer to an fsimage
    oiv_legacy           apply the offline fsimage viewer to an legacy fsimage
    oev                  apply the offline edits viewer to an edits file
    fetchdt              fetch a delegation token from the NameNode
    getconf              get config values from configuration
    groups               get the groups which users belong to
    snapshotDiff         diff two snapshots of a directory or diff the
                           current directory contents with a snapshot
    lsSnapshottableDir   list all snapshottable dirs owned by the current user
                                Use -help to see options
    portmap              run a portmap service
    nfs3                 run an NFS version 3 gateway
    cacheadmin           configure the HDFS cache
    crypto               configure HDFS encryption zones
    storagepolicies      list/get/set block storage policies
    version              print the version
    Most commands print help when invoked w/o parameters.
```
在上面的列表中，第一个dfs是经常被使用的命令。可以通过hdfs dfs-help查看dfs的具体使用方法。dfs命令，就是通过命令行操作HDFS目录或是文件 的命令，类似于Linux文件命令一样，只不过dfs操作的是HDFS文件系统中的文件。下表列出HDFS几个常用命令。
![](../../attachments/Pasted%20image%2020241010215408.png)

完成以下示例：

显示根目录下的所有文件和目录：
创建目录
```
hdfs dfs -mkdir /test
```

删除目录
```
hdfs dfs -rm -r /data
```

```
hdfs dfs -ls /
```
![](../../attachments/Pasted%20image%2020241010220127.png)

以递归的形式显示根目录下的所有文件或目录，注意-R参数：
```
hdfs dfs -ls -R /
```
![](../../attachments/Pasted%20image%2020241010220303.png)

将本地文件上传到HDFS上：

共享文件夹创建a.txt，以下为内容
```
aaaa
bbb
fdryhthrjrj
hgrthtrhth
gfngfchdaw4
th
gfg
hth
fch
bag
```

创建test，使用cp命令将a.txt 复制到 / 下
```
cd /
mkdir /test
cp /mnt/hgfs/sharevm/a.txt /test/a.txt
cp /test/a.txt
```

将本地文件上传到HDFS上：
```
hdfs dfs -copyFromLocal a.txt /tmp/a.txt
```

删除HDFS上的文件：
```
hdfs dfs -rm /tmp/a.txt
```

使用put命令，同样可以将本地文件上传到HDFS上：
```
hdfs dfs -put a.txt /tmp/b.txt
```

使用get/copyToLocal选项，可以下载文件到本地：
```
hdfs dfs -get /tmp/b.txt a.txt
hdfs dfs -copyToLocal /tmp/b.txt a1.txt
```
![](../../attachments/Pasted%20image%2020241010223433.png)

### 2.2 通过Java项目访问HDFS
不仅可以使用hdfs命令操作HDFS文件系统上的文件，还可以使用Java代码访问HDFS文件系统中的文件。在Java代码中，操作HDFS主要通过以下几个主要的类：

● Configuration：用于配置HDFS。

● FileSystem：表示HDFS文件系统。

● Path：表示目录或是文件路径。

首先须在window中配置java环境

1.JAVA jdk安装教程：
https://blog.csdn.net/weixin_47406082/article/details/133418026

![](../../attachments/Pasted%20image%2020241010225615.png)

2.Maven安装教程：Maven的安装与配置及IDEA配置（超详细图文讲解）_maven安装及配置教程-CSDN博客
[Maven的安装与配置及IDEA配置（超详细图文讲解）_maven安装及配置教程-CSDN博客](https://blog.csdn.net/weixin_57367513/article/details/126768909)

[‘mvn‘ 不是内部或外部命令，也不是可运行的程序 或批处理文件。_mvn' 不是内部或外部命令,也不是可运行的程序 或批处理文件。-CSDN博客](https://blog.csdn.net/will__be/article/details/117412180)
![](../../attachments/Pasted%20image%2020241010232250.png)

使用IDEA创建Java项目来操作HDFS文件系统。IDEA有两个版本IC和IU两个版本，其中IU为IDEA Ultimate为完全功能版本，此版本需要付费后才能使用，可以选择IC版本， C为Community即社区版本的意思，IC版本为免费版本。后面集成开发环境将选择此IDEA Community版本。IDEA的下载地址为https://www.jetbrains.com/idea/download/#section=windows。选择下载IDEA Community版本，如图所示。

创建maven 项目：[在IDEA 2024.1.3 (Community Edition)中创建Maven项目_idea 社区版建立maven项目-CSDN博客](https://blog.csdn.net/Chasingthewinds/article/details/139757913)

为了方便管理，以模块方式来开发，每一个实验可以为一个模块，而Hadoop是这些模块的父项目。所以，在创建完成Hadoop项目后，修改Hadoop的项目类型为pom。以下Hadoop父项目的pom.xml文件部分内容，父项目的\<package>类型为pom。

【代码】hadoop/pom.xml文件
```
    <groupId>org.hadoop</groupId>
    <artifactId>hadoop</artifactId>
    <version>1.0</version>
    <packaging>pom</packaging>
```

在父项目中的dependencyManagement添加所需要的依赖后，子模块只需要添加依赖名称，不再需要导入依赖的版本。这样父项目就起到了统一管理版本的功能。

```
    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.apache.hadoop</groupId>
                <artifactId>hadoop-client</artifactId>
                <version>3.2.2</version>
            </dependency>
            <dependency>
                <groupId>junit</groupId>
                <artifactId>junit</artifactId>
                <version>4.13.2</version>
            </dependency>
        </dependencies>
    </dependencyManagement>
```

再创建第一个模块。选择Hadoop项目，选择创建模块，如图所示。
![](../../attachments/Pasted%20image%2020241012225633.png)

在创建的子模块chapter02中，修改pom.xml文件，添加以下依赖。注意，只输入groupId和artifactId，不需要输入版本，因为Hadoop项目作为父项目管理依赖的版本。
```
    <dependencies>
       <dependency>
           <groupId>org.apache.hadoop</groupId>
           <artifactId>hadoop-client</artifactId>
       </dependency>
       <dependency>
           <groupId>junit</groupId>
           <artifactId>junit</artifactId>
           <scope>test</scope>
       </dependency>
   </dependencies>
```

查看hadoop client Aggretator的依赖关系，如图所示。


至此已经可以开发Java代码，访问HDFS文件系统了。

步骤02 HDFS操作示例。

(1)显示HDFS指定目录下的所有目录

显示所有目录，使用fileSystem.listStatus方法。

【代码】Demo01AccessHDFS.java
```java
package org.hadoop;
 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import java.io.IOException;
import org.apache.hadoop.fs.FileStatus;
 
public class Demo01AccessHDFS {
 
    public static void main(String[] args) {
        System.setProperty("HADOOP_USER_NAME", "hadoop");
        Configuration config = new Configuration();
        config.set("fs.defaultFS", "hdfs://192.168.56.201:8020");//注意修改IP
        FileSystem fs = null;
        try {
            fs = FileSystem.get(config);
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
        FileStatus[] stas = null;
        try {
            stas = fs.listStatus(new Path("/"));
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
        for (FileStatus f : stas) {
            System.out.println(f.getPermission().toString()+" "
                    +f.getPath().toString());
        }
        try {
            fs.close();
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }
}
```

运行结果：
![](../../attachments/Pasted%20image%2020241016224325.png)

代码说明：
● 第1行代码用于设置访问Hadoop的用户名。
● 第2行代码用于声明一个新的访问配置对象。
● 第3行代码设置访问的具体地址。
● 第4行代码创建一个文件系统对象。
● 第5行~8行代码为输出根目录下的所有文件或目录，不包含子目录。
● 第9行代码关闭文件系统。

(2)显示所有文件显示所有文件，使用fileSystem.listFiles函数，第二个参数boolean用于指定是否递归显示所有文件。
【代码】Demo02ListFiles.java

```java
package org.hadoop;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.FileSystem;  
import org.apache.hadoop.fs.LocatedFileStatus;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.fs.RemoteIterator;  
import java.io.IOException;  
  
public class Demo02ListFiles {  
    public static void main(String[] args) {  
        FileSystem fs = null;  
        try {  
            // 设置 Hadoop 用户名  
            System.setProperty("HADOOP_USER_NAME", "hadoop");  
            // 创建 Hadoop 配置对象  
            Configuration config = new Configuration();  
            config.set("fs.defaultFS", "hdfs://192.168.95.130:8020");//注意修改IP  
            // 获取 FileSystem 实例  
            fs = FileSystem.get(config);  
            // 列出 HDFS 上的文件  
            RemoteIterator<LocatedFileStatus> files = fs.listFiles(new Path("/"), true);  
            // 遍历并输出文件信息  
            while (files.hasNext()) {  
                LocatedFileStatus file = files.next();  
                System.out.println(file.getPermission() + " " + file.getPath());  
            }  
        } catch (IOException e) {  
            e.printStackTrace();  
        } finally {  
            // 确保在完成后关闭 FileSystem 实例  
            try {  
                if (fs != null) fs.close();  
            } catch (IOException e) {  
                e.printStackTrace();  
            }  
        }  
    }  
}

```

运行结果：
![](../../attachments/Pasted%20image%2020241016232831.png)

(3)读取HDFS文件的内容

读取HDFS上的内容，可以和fileSystem.open(...)打开一个文件输入流，然后读取文件流中的内容即可。

【代码】Demo03ReadFile.java
```java
package org.hadoop;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.FileSystem;  
import org.apache.hadoop.fs.Path;  
import java.io.DataInputStream;  
import java.io.IOException;  
  
public class Demo03ReadFile {  
    public static void main(String[] args) {  
//        config.set("fs.defaultFS", "hdfs://192.168.95.130:8020");//注意修改IP  
        String server = "hdfs://192.168.95.130:8020";  
        String filePath = "/test/a.txt"; // 文件在 HDFS 中的路径  
  
        System.setProperty("HADOOP_USER_NAME", "hadoop");  
  
        Configuration config = new Configuration();  
        config.set("fs.defaultFS", server);  
  
        try (FileSystem fs = FileSystem.get(config);  
             DataInputStream in = fs.open(new Path(filePath))) {  
  
            byte[] bs = new byte[1024];  
            int len;  
            while ((len = in.read(bs)) != -1) {  
                String str = new String(bs, 0, len);  
                System.out.print(str);  
            }  
        } catch (IOException e) {  
            e.printStackTrace();  
        }  
    }  
}
```

运行结果
![](../../attachments/Pasted%20image%2020241016233245.png)


(4)向HDFS写入数据

向HDFS写入数据，可以使用fileSysten.create/append方法，获取一个OutputStream，然后向里面输入数据即可。

【代码】Demo04WriteFile.java

```java
package org.hadoop;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.FileSystem;  
import org.apache.hadoop.fs.Path;  
import java.io.IOException;  
import java.io.OutputStream;  
  
public class Demo04WriteFile {  
    public static void main(String[] args) {  
        String server = "hdfs://192.168.95.130:8020";  
        System.setProperty("HADOOP_USER_NAME", "hadoop"); // 设置 HDFS 用户  
  
        Configuration config = new Configuration();  
        config.set("fs.defaultFS", server);  
  
        try (FileSystem fs = FileSystem.get(config)) {  
            Path filePath = new Path("/test/b.txt"); // 文件路径  
            try (OutputStream out = fs.create(filePath)) {  
                out.write("Hello Hadoop\n".getBytes());  
                out.write("中文写入测试\n".getBytes());  
                // No need to explicitly close OutputStream, it's handled by try-with-resources  
            } catch (IOException e) {  
                e.printStackTrace();  
            }  
        } catch (IOException e) {  
            e.printStackTrace();  
        }  
    }  
}

```

运行结果
![](../../attachments/Pasted%20image%2020241016233658.png)

代码输入完成以后，通过cat查看文件中的内容：
```
hdfs dfs -cat /test/b.txt
```

![](../../attachments/Pasted%20image%2020241017224225.png)

[修改CentOS7，修改默认语言环境，解决中文乱码问题。_centos7修改lang.sh-CSDN博客](https://blog.csdn.net/liguangxianbin/article/details/79814964)

[当Linux配置zh_CN.UTF-8 ，中文还是显示乱码解决办法_服务器中文编码设置-CSDN博客](https://blog.csdn.net/u012949658/article/details/128780244)

[XShell免费版的安装配置教程以及使用教程（超级详细、保姆级）-CSDN博客](https://blog.csdn.net/m0_67400972/article/details/125346023)

### 2.3 使用winutils解决警告信息
Hadoop通常运行在Linux上，而开发程序通常是Windows，执行代码时，为了看到更多的日志信息，需要添加log4j.properties或log4j2的log4j2.xml。通过查看hadoop-client-3.2.2依赖可知，系统中已经包含了log4j 1.2的日志组件，如图所示。

![](../../attachments/Pasted%20image%2020241017224853.png)
此时只需要添加一个log4j与slf4j整合的依赖即可，所以添加以下依赖：
![](../../attachments/Pasted%20image%2020241017224916.png)

![](../../attachments/Pasted%20image%2020241017225640.png)

## 3. 实现MapReduce的WordCount
3.1 MapReduce的运算过程
MapReduce为分布式计算模型，分布式计算最早由Google提出。MapReduce将运算的过程分为两个阶段，map和reduce阶段。用户只需要实现map和reduce两个函数即可。这两个函数参数的形式都是以Key-Value的形式成对出现，Key为输出或输出的信息，Value为输入或输出的值。

如图展示了MapReduce的运算过程。将大任务交给多个机器分布式进行计算，然后再进行汇总合并。

### 3.2 WordCount示例
以本地运行和服务器运行的方式分别部署，能更深入了解MapReduce的开发、运行和部署。

使用打包的方式将程序打包后放到Hadoop集群上运行。

步骤01 创建Java项目并添加依赖。

创建Java项目，并添加以下依赖。注意，本次以添加的依赖为hadoop-mincluster，且设置scope的值为provided（意思是，在打包时将不会被打包到依赖的jar包中）​。



步骤02 开发WordCount的完整代码。

【代码】WordCount.java
```java
package org.hadoop;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.conf.Configured;  
import org.apache.hadoop.fs.FileSystem;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.Reducer;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import org.apache.hadoop.util.Tool;  
import org.apache.hadoop.util.ToolRunner;  
import java.io.IOException;  
public class WordCount extends Configured implements Tool {  
    public static void main(String[] args) throws Exception {  
        int result = ToolRunner.run(new WordCount(), args);  
        System.exit(result);  
    }  
    //声明嗠顺地址  
    private static String server = "hdfs://192.168.95.130:8020";  
//    private static String server = "hdfs://server130:8020";  
    public int run(String[] args) throws Exception {  
        if (args.length != 2) {  
            System.err.println("usage: " + this.getClass().getSimpleName() + " <inPath> <outPath>");  
            ToolRunner.printGenericCommandUsage(System.out);  
            return -1;  
        }  
        Configuration config = getConf();  
        config.set("fs.defaultFS", server);  
        //指定resourcemanger的地址  
        config.set("yarn.resourcemanager.hostname", "server130");  
        config.set("dfs.replication", "1");  
        config.set("dfs.permissions.enabled", "false");  
        FileSystem fs = FileSystem.get(config);  
        Path dest = new Path(server + args[1]);  
        if (fs.exists(dest)) {  
            fs.delete(dest, true);  
        }  
        Job job = Job.getInstance(config,"WordCount");  
        job.setJarByClass(getClass());  
        job.setMapperClass(WordCountMapper.class);  
        job.setReducerClass(WordCountReducer.class);  
        job.setOutputKeyClass(Text.class);  
        job.setOutputValueClass(LongWritable.class);  
        FileInputFormat.addInputPath(job, new Path(server + args[0]));  
        FileOutputFormat.setOutputPath(job, dest);  
        boolean boo = job.waitForCompletion(true);  
        return boo ? 0 : 1;  
    }  
  
    public static class WordCountMapper extends Mapper<LongWritable, Text, Text, LongWritable> {  
        private LongWritable count = new LongWritable(1);  
        private Text text = new Text();  
        @Override  
        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
            String str = value.toString();  
            String[] strs = str.split("\\s+");  
            for (String s : strs) {  
                text.set(s);  
                context.write(text, count);  
            }  
        }  
    }  
  
    public static class WordCountReducer extends Reducer<Text, LongWritable, Text, LongWritable> {  
        @Override  
        public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {  
            long sum = 0;  
            for (LongWritable w : values) {  
                sum += w.get();  
            }  
            context.write(key, new LongWritable(sum));  
        }  
    }  
}
```

上例代码中，由于我们声明了完整的地址，所以可以在本地运行测试。在本地运行测试需要输入两个参数。选择IDEA的run > Edit Configurations，并在Program Arguments位置输入读取文件的地址和输出结果的目录，如图所示。




![](../../attachments/Pasted%20image%2020241018190753.png)

步骤03 使用Maven打包程序。

在IDEA右侧栏的Maven视图中，单击package并运行，可以得到一个jar包，如图所示。
![](../../attachments/Pasted%20image%2020241018191107.png)

打完的包可以在target目录下找到，将jar包上传到server201服务器的/root目录下，并使用yarn jar执行。
![](../../attachments/Pasted%20image%2020241018191552.png)

使用yarn jar执行，使用以下命令：
```
yarn jar chapter02.jar org.hadoop.WordCount /test/a.txt /out002
```

![](../../attachments/Pasted%20image%2020241018191745.png)

### 3.3可能遇到的bug：
[org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z的解决办法-CSDN博客](https://blog.csdn.net/syl_ccc/article/details/105946007)

![|350](../../attachments/Pasted%20image%2020241018190840.png)

**其他方案：**
注意：在本地运行时，有可能会出现以下错误：

Exception in thread "main" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.Nati veIO$Windows.access0(Ljava/lang/String;I)Z

解决方案是：将hadoop.dll文件放到windows/system32目录下即可。

## 4.Hbase安装配置实验
HBase是Hadoop DataBase的意思。HBase是一种构建在HDFS之上的分布式、面向列的存储系统。在需要实时读写、随机访问超大规模数据集时，可以使用HBase。HBase是Google Bigtable的开源实现，与Google Bigtable利用GFS作为其文件存储系统类似，HBase利用Hadoop HDFS作为其文件存储系统，也是利用HDFS实现分布式存储的。Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据，Google Bigtable利用Chubby作为协同服务，HBase利用ZooKeeper作为协同服务。

HBase具有如下特点：

● 大：一个表可以有上亿行，上百万列。

● 面向列：面向列表（族）的存储和权限控制，列（族）独立检索。

● 稀疏：对于为空(NULL)的列，并不占用存储空间，因此表可以设计得非常稀疏。

● 无模式：每一行都有一个可以排序的主键和任意多的列，列可以根据需要动态增加，同一张表中不同的行可以有截然不同的列。

● 数据多版本：每个单元中的数据可以有多个版本。默认情况下，版本号自动分配，版本号就是单元格插入时的时间戳。

● 数据类型单一：HBase中的数据都是字符串，没有类型。

### 4.1 HBase的安装
HBase可能独立运行在一台主机上。此时HBase将只会有一个进程，即HMaster（这个HMaster内含HRegionServer和HQuorumPeer两个服务）​。虽然只有一个进程，但也可以提供HBase的大部分功能。首先需要了解HBase的版本信息，可以在HBase的官方网站上，通过hbase documents查看HBase对JDK、Hadoop版本的兼容性，尤其是对Hadoop版本具有较强依耐性。下载的地址为
https://mirrors.tuna.tsinghua.edu.cn/apache/hbase

4.1.1 HBase的单节点安装
HBase单节点安装可以快速学会HBase的基本使用。HBase的单节点安装不需要Hadoop，数据保存到指定的磁盘目录下。在hbase-site.xml文件中，通过“hbase.rootdir=file://” 来指定数据保存的目录。也不需要ZooKeeper，启动后HBase将使用自带的ZooKeeper。只有一个进程Hmaster（内部包含：HRegionServer和HQuorumPeerman两个子线程）​。以下是HBase单节点安装的过程。

步骤01 上传并解压HBase（通过winscp）。

同样地，使用tar将HBase解压到/app/目录下：

```
tar -zxvf mnt/hgfs/sharevm/hbase-2.4.18-bin.tar.gz -C /app/
```
![](../../attachments/Pasted%20image%2020241018195615.png)

步骤02 修改配置文件。首先修改的是hbase-env.sh文件，此文件中保存了JAVA_HOME环境变量信息。配置如下：

```
vim /app/hbase-2.4.18/conf/hbase-env.sh
```
添加以下信息
```
export JAVA_HOME=/usr/java/jdk1.8.0_421
export HBASE_MANAGES_ZK=true
```

再修改hbase-site.xml配置文件，此配置文件中，需要指定HBase数据的保存目录，由于目的是快速入门，所以可以先将HBase的数据保存到磁盘上。具体的配置如下：

```
vim /app/hbase-2.4.18/conf/hbase-site.xml
```

```
<configuration>
	<property>
		<name>hbase.rootdir</name>
		<value>file:///app/datas/hbase</value>
	</property>
	<property>
		<name>hbase.zookeeper.property.dataDir</name>
		<value>/app/datas/zookeeper</value>
	</property>
	<!--以下配置是否检查流功能-->
	<property>
		<name>hbase.unsafe.stream.capability.enforce</name>
		<value>false</value>
	</property>
</configuration>
```

修改regionservers文件，此文件用于指定HRegionServer的服务器地址，由于是单机部署，所以指定本机名称即可。添加本机主机名：

```
vim regionservers

```

步骤03 启动／停止HBase。

启动HBase，只需要在HBase的bin目录下，执行start-hbase.sh即可。

```
/app/hbase-2.4.18/bin/start-hbase.sh
```

![](../../attachments/Pasted%20image%2020241018201159.png)

启动完成以后，通过jps查看进程，会发现HMaster进程已经运行：
```
jps
```

![](../../attachments/Pasted%20image%2020241018201242.png)

停止HBase只需要执行stop-hbase.sh即可：

```
/app/hbase-2.4.18/bin/stop-hbase.sh
```

步骤04 登录HBase Shell。

在bin目录下，使用hbase shell命令登录HBase Shell，即可操作HBase数据库。登录成功后，将显示hbase >命令行。

```
/app/hbase-2.4.18/bin/hbase shell
```

![](../../attachments/Pasted%20image%2020241018201455.png)

步骤05 HBase数据操作。

快速创建一个表，保存一些数据，以了解HBase是如何保存数据的。首先，如果不了解HBase命令，可以直接在HBase的命令行输入help，此命令将会显示HBase的帮助信息。

创建一个命名空间，可以理解成创建了一个数据库：

```
hbase(main):009:0> create_namespace 'ns1'

Took 0.1776 seconds
```
查看所有命名空间，可以理解成查看所有数据库：
```
    hbase(main):010:0> list_namespace
    NAMESPACE
    default
    hbase
    ns1
    3 row(s)
    Took 0.0158 seconds
```

![](../../attachments/Pasted%20image%2020241018201642.png)

创建一个表，在指定的命名空间下，并指定列族为“f”​。可以理解为创建一个表，并指定一个列名为f：
```
    hbase(main):011:0> create 'ns1:stud','f'
    Created table ns1:stud
    Took 0.8601 seconds
    => Hbase::Table - ns1:stud
```

向表中写入一行记录，其中R001为主键，即Row Key，f:name为列名，Jack为列值。此处与关系型数据库有很大的区别，注意区分。
```
    hbase(main):012:0> put 'ns1:stud','R001','f:name','Jack'
    Took 0.3139 seconds
    hbase(main):013:0> put 'ns1:stud','R002','f:age','34'
    Took 0.0376 seconds
```

![](../../attachments/Pasted%20image%2020241018201825.png)

查询表，类似于关系型数据库中的select：
```
    hbase(main):014:0> scan 'ns1:stud'
    ROW                              COLUMN+CELL
    R001                  column=f:name, timestamp=1568092417316, value=Jack
    R002                  column=f:age, timestamp=1568092435076, value=34
    2 row(s)
    Took 0.0729 seconds
```

![](../../attachments/Pasted%20image%2020241018201929.png)

### 4.1.2 HBase的伪分布式安装
HBase的伪分布式安装，是将HBase的数据保存到伪分布式的HDFS系统中。此时Hadoop环境为伪分布式，HBase的节点只有一个，HBase使用独立运行的ZooKeeper。以下步骤将配置一个HBase的伪分布式运行环境。需要：

● 配置好的Hadoop运行环境。

● 独立运行的ZooKeeper，只有一个节点即可。

● 将Hadoop的ZooKeeper指向这个独立的ZooKeeper。

#### 步骤01 准备Hadoop和ZooKeeper。

##### 步骤01 配置zoo.cfg。

zookeeper 配置

配置好Hadoop的伪分布式运行环境。

配置好ZooKeeper的独立节点运行环境：ZooKeeper集群安装-CSDN博客。[ZooKeeper集群安装-CSDN博客](https://blog.csdn.net/qq_53877854/article/details/142580659?spm=1001.2014.3001.5502)

[Linux安装Zookeeper详细步骤（下载，安装，配置，启动，停止）_zkcli.sh下载-CSDN博客](https://blog.csdn.net/qq_54796785/article/details/126136264)

下载并解压到/app/
```
tar -zxvf /mnt/hgfs/sharevm/apache-zookeeper-3.9.2-bin.tar.gz -C /app/
```

```
cp /app/apache-zookeeper-3.9.2-bin/conf/zoo_sample.cfg /app/apache-zookeeper-3.9.2-bin/conf/zoo.cfg
```

```
vim /app/apache-zookeeper-3.9.2-bin/conf/zoo.cfg
```

注释掉原有的dataDir，添加以下:
1. 
```
#配置ZooKeeper数据保存目录

dataDir=/app/datas/zk
```

2. 配置ZooKeeper集群：
```
server.1=192.168.95.130:2888:3888

server.2=192.168.95.131:2888:3888

server.3=192.168.95.132:2888:3888
```

##### 步骤02 使用scp将文件发送到其他两台机器。

```
scp -r /app/apache-zookeeper-3.9.2-bin server131:/app/

scp -r /app/apache-zookeeper-3.9.2-bin server132:/app/
```

然后修改每一个dataDir目录下的myid文件。在server101主机上的myid中添加101（为了方便记忆才取这个ID的，此ID等于IP的地址，建议可以从1开始）​，即：

每个服务器创建myid
```
mkdir /app/datas/zk/
```

```
touch /app/datas/zk/myid
```

```
echo 130 > /app/datas/zk/myid
```

配置环境
```
vim /etc/profile
```


##### 步骤03 现在分别启动三台主机的ZooKeeper。
![](../../attachments/Pasted%20image%2020241018210206.png)

启动命令：
```
/app/apache-zookeeper-3.9.2-bin/bin/zkServer.sh start
```

```
/app/apache-zookeeper-3.9.2-bin/bin/zkServer.sh stop
```

![](../../attachments/Pasted%20image%2020241018214252.png)

打开失败的话

添加以下
```
#zookeeper
export ZK_HOME=/app/apache-zookeeper-3.9.2-bin
export PATH=$PATH:$ZK_HOME/bin
```

删除以下文件并重启
```
rm -rf /app/datas/zk/version-2
rm -rf /app/datas/zk/zookeeper_server.pid
```

然后查看状态，使用status检查状态：
```
/app/apache-zookeeper-3.9.2-bin/bin/zkServer.sh status
```
![](../../attachments/Pasted%20image%2020241018215134.png)

步骤04 测试操作。

登录客户端：
```
/app/apache-zookeeper-3.9.2-bin/bin/zkCli.sh
```
![](../../attachments/Pasted%20image%2020241018215255.png)

创建一个新的目录，且写入数据：
```
create /test TestData
```

再次显示当前根目录下的所有数据。登录其他主机，如果查看到相同的结果，即表示已经同步。
![](../../attachments/Pasted%20image%2020241018215431.png)

ZooKeeper的命令很多，可以使用help查看所有可使用的命令。
```
help
```

步骤05 创建znode节点

znode的节点类型分为持久节点、顺序节点和临时节点。默认创建的节点都是持久节点，使用-s参数可以创建一个顺序节点，使用-e参数可以创建临时节点。

创建顺序节点：
```
create -s /t ""
```

![](../../attachments/Pasted%20image%2020241018215600.png)

创建临时节点，客户端退出时自动被删除：
```
create -e /tt ""
```
![](../../attachments/Pasted%20image%2020241018215644.png)

步骤06 观察节点

观察者ObServer可以观察到节点的变化，常用命令如下：
```
stat path [watch]

ls path [watch]

ls2 path [watch]

get path [watch]
```

上面的命令后面都有一个[watch]​，即可以在命令行观察节点的变化。首先通过某个设置，在后面直接添加watch：
```
ls /one watch
```

然后在另一个的客户端，添加一个子节点：
```
create /one/1
```

现在可以观察到节点显示的数据：
![](../../attachments/Pasted%20image%2020241018220513.png)
#### 步骤02 修改HBase配置文件。

首先修改HBase的配置文件hbase-env.sh文件，在此配置文件中，重点配置ZooKeeper选项，配置为使用外部的ZooKeeper即可。具体配置如下：

```
vim /app/hbase-2.4.18/conf/hbase-env.sh
```

```
export JAVA_HOME=/usr/java/jdk1.8.0_421

export HBASE_MANAGES_ZK=false
```

修改HBase配置文件hbase-site.xml文件，重点关注hbase.rootdir用于指定在Hadoop中存储HBase数据的目录。ZooKeeper用于指定ZooKeeper地址。
```
vim /app/hbase-2.4.18/conf/hbase-site.xml
```

```xml
    <configuration>
        <property>
            <name>hbase.cluster.distributed</name>
            <value>true</value>
        </property>
        <property>
            <name>hbase.tmp.dir</name>
            <value>/app/datas/hbase/tmp</value>
        </property>
        <property>
            <name>hbase.unsafe.stream.capability.enforce</name>
            <value>false</value>
        </property>
        <property>
            <name>hbase.rootdir</name>
            <value>hdfs://server130:8020/hbase</value>
        </property>
        <property>
            <name>hbase.zookeeper.quorum</name>
            <value>server130:2181</value>
        </property>
    </configuration>
```

**启动服务**

启动hadoop
```
/app/hadoop-3.4.0/sbin/start-all.sh
```

启动hbase
```
/app/hbase-2.4.18/bin/start-hbase.sh
```

启动zookeeper
```
/app/apache-zookeeper-3.9.2-bin/bin/zkServer.sh start
```

**关闭服务**
启动hadoop
```
/app/hadoop-3.4.0/sbin/stop-all.sh
```

启动hbase
```
/app/hbase-2.4.18/bin/stop-hbase.sh
```

启动zookeeper
```
/app/apache-zookeeper-3.9.2-bin/bin/zkServer.sh stop
```


![](../../attachments/Pasted%20image%2020241018221956.png)\

[stop-hbase.sh关闭不了，一直处于等待状态。（已解决）_stophbasesh一直处于等待状态-CSDN博客](https://blog.csdn.net/weixin_45890771/article/details/122205099)

![](../../attachments/Pasted%20image%2020241018231721.png)
[【hadoop】解决浏览器不能访问Hadoop的50070、8088等端口_hadoop50070访问不了-CSDN博客](https://blog.csdn.net/xiangxiang613/article/details/119939224)
