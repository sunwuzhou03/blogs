# 第一届开悟全国公开赛高级赛道
# 今年3v3赛题的难点

决赛涉及到多智能体解决方案（更强调AI之间的配合协作），并且比赛使用的游戏环境具有更大的状态空间，需要更复杂的模型结构与强化学习算法。此外，参赛选手还要考虑奖励函数设计，训练方式探索等内容。
**难点**：如何设计网络结构进行多阵营的训练？如何在奖励的调整和设计上让不同英雄之间进行合作？

**网络结构设计**：由于今年可以同时执行两个训练任务，并且可以集成多个模型，我们针对多阵营提出了两套解决方案，方案一：搭建分支网络，每个分支网络训练一种类型的英雄，比如打野分支网络只训练打野类型的英雄，为了做到这一点，除了重新搭建网络结构，还需要改变特征输入的顺序，即把打野的特征输入到打野分支网络，射手特征输入到射手分支网络。方案二：在baseline网络结构基础上进行少量的改动，保持轻量级网络结构的基础上直接进行多阵营的训练，使智能体能够更快的学会各种操作，然后在智能体掌握基本能力基础上继续进行单阵阵容的训练，最后将三个阵容进行集成。训练过程中，我们发现分支网络的效果并不是很理想，收敛速度慢于另外的一个轻量级的网络，而轻量级网络也在前期训练中很快达到了与baseline8持平的效果，所以在后期我们就集中在轻量级网络的训练上。

**奖励调整与设计**：同样采取两套方案，方案一：不改变奖励函数，只根据训练的阶段调整奖励系数。方案二：我们参考往年清水河畔混分王队伍的情景奖励重新编写奖励函数，并添加了一些瞬时奖励和时序奖励，包括击杀协助奖励，大龙击杀奖励，集火时序奖励。该奖励使用在后期单阵营训练上。效果上仅仅使用方案一的情况下，模型能够和baseline8实力持平，方案二实际使用中由一定的效果，但是并没有做充足的对比验证。

# 如果明年参加比赛，对比赛的期待？

希望比赛的在线ide能够提供更多的自由度，比如在在线ide上能够进行模型参数融合的操作，模型集成上能进行直接集成，这次比赛想进行模型集成还是不太方便，需要在执行训练任务并训练一段时间，才能导入其他模型。同时也很期下一届比赛能够开放更多的玩法，比如5v5和人机博弈，能够有更多的队伍参与到开悟比赛里！
